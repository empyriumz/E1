{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dcaf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/Profluent-AI/E1.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cuda_capabilities = torch.cuda.get_device_capability(0)\n",
    "    if cuda_capabilities[0] >= 8:\n",
    "        print(\"CUDA 8.0 or higher detected; installing flash-attention\")\n",
    "        !pip install flash-attn --no-build-isolation\n",
    "    else:\n",
    "        print(\"CUDA capability lower than 8.0; will not be using flash attention\")\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18151867",
   "metadata": {},
   "source": [
    "### Embedding Adenylate Kinase variants\n",
    "\n",
    "In this notebook, we will use E1 model in single sequence mode to compute embeddings of variants of a protein called Adenylate Kinase. Each variant of this protein has a specific structural feature called lid type and we will see if the protein embeddings can be used to cluster the variants based on their lid type. The data for this is taken from repository https://github.com/keiserlab/face-plm. We will use the `E1Predictor` tool to efficiently compute and return these embeddings.\n",
    "\n",
    "NOTE: You can also use `python3 -m E1.tools.predict` to compute embeddings for a list of sequences stored as fasta file. Use `torchrun --nproc-per-node=gpu -m E1.tools.predict` to run it on multiple GPUs. See file for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8963a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "data = pl.read_csv(\n",
    "    \"https://raw.githubusercontent.com/keiserlab/face-plm/refs/heads/main/data/adk_evo-scale_dataset.csv\"\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import E1.dist as dist\n",
    "from E1.modeling import E1ForMaskedLM\n",
    "from E1.predictor import E1Predictor\n",
    "\n",
    "model_name = \"Profluent-Bio/E1-300m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a467d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = E1ForMaskedLM.from_pretrained(model_name, dtype=torch.float).to(dist.get_device()).eval()\n",
    "predictor = E1Predictor(\n",
    "    model=model,\n",
    "    max_batch_tokens=16384,  # Change to 4096 on Colab T4 GPU\n",
    "    # We save token_embeddings of shape (Sequence Length, Embedding Dim) for all positions in the sequence.\n",
    "    # and mean_token_embeddings of shape (Embedding Dim, ) which is the mean of token_embeddings over all positions.\n",
    "    fields_to_save=[\"token_embeddings\", \"mean_token_embeddings\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_embeddings = []\n",
    "\n",
    "for prediction in predictor.predict(\n",
    "    sequences=data[\"sequence\"].to_list(), sequence_ids=data[\"org_name\"].to_list(), context_seqs=None\n",
    "):\n",
    "    # Note, predictions may not be in the same order as the input sequences due to batching by length\n",
    "    # Use prediction[\"id\"] to match with the input sequences which is set to sequence id.\n",
    "    org_name = prediction[\"id\"]\n",
    "    token_embeddings = prediction[\"token_embeddings\"]  # (Sequence Length, Embedding Dim)\n",
    "    mean_token_embeddings = prediction[\"mean_token_embeddings\"]  # (Embedding Dim, )\n",
    "    sequence_embeddings.append({\"org_name\": org_name, \"sequence_embedding\": mean_token_embeddings.cpu().numpy()})\n",
    "\n",
    "sequence_embeddings = pl.DataFrame(sequence_embeddings).join(\n",
    "    data.select([\"org_name\", \"lid_type\"]), on=\"org_name\", how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44c159",
   "metadata": {},
   "source": [
    "We will use UMAP to reduce the dimensionality of the embeddings to 2D and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0026ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ddb783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from umap import UMAP\n",
    "\n",
    "sequence_embeddings_array = np.array(sequence_embeddings[\"sequence_embedding\"].to_list())\n",
    "lid_types = sequence_embeddings[\"lid_type\"].to_list()\n",
    "\n",
    "umap = UMAP(n_components=2, random_state=314)\n",
    "sequence_embeddings_2d = umap.fit_transform(sequence_embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2478a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=sequence_embeddings_2d[:, 0], y=sequence_embeddings_2d[:, 1], hue=lid_types)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aebd2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
